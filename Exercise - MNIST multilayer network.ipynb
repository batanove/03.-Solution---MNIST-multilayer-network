{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST multilayer network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise - Load and preprocess data\n",
    "\n",
    "> **Exercise**: Load the MNIST data. Split it into train, validation and test sets. Standardize the images. Define a `get_batches(X, y, batch_size)` function to generate random X/y batches of size `batch_size` using a Python generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (10000, 784) uint8\n",
      "y: (10000,) int32\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Load data\n",
    "with np.load(os.path.join('data', 'mnist-10k.npz')) as npz_file:\n",
    "      # Load items into a dictionary\n",
    "    X = npz_file['data']\n",
    "    y = npz_file['labels']\n",
    "    \n",
    "print('X:', X.shape, X.dtype)\n",
    "print('y:', y.shape, y.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Create train/test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    # Convert uint8 pixel values to float\n",
    "    X.astype(np.float32),\n",
    "    y,\n",
    "    test_size=1000, random_state=0\n",
    ")\n",
    "\n",
    "# Split test into validation/test sets\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(\n",
    "    X_test, y_test,\n",
    "    test_size=500, random_state=0\n",
    ")\n",
    "\n",
    "# Standardize\n",
    "#scaler = StandardScaler()\n",
    "#X_train = scaler.fit_transform(X_train)\n",
    "#X_valid = scaler.transform(X_valid)\n",
    "#X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to get batches of data\n",
    "def get_batches(X, y, batch_size):\n",
    "    # Enumerate indexes by steps of batch_size\n",
    "    # i: 0, b, 2b, 3b, 4b, .. where b is the batch size\n",
    "    for i in range(0, len(y), batch_size):\n",
    "        # \"yield\" data between index i and i+b (not included)\n",
    "        yield X[i:i+batch_size], y[i:i+batch_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise - Create and train a multilayer network\n",
    "\n",
    "> **Exercise:** Create a multilayer neural network and train it using your batch generator. Evaluate the accuracy on the validation set after each epoch. Test different architectures and parameters. Evaluate your best network on the test set. Save the trained weights of the first fully connected layer in a variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###SetUp TensorFlow Graph (followin explanation from here: https://www.youtube.com/watch?v=XxFYGey6YxM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "X = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Neural Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters\n",
    "nr_epochs = 5\n",
    "learning_rate = 0.0001\n",
    "#2 hidden layers and number of neurons\n",
    "n_hidden1 = 512\n",
    "n_hidden2 = 64\n",
    "\n",
    "#setting of initial weight 1 (784 is the total amount of inital inputs)\n",
    "initial_w1 = tf.truncated_normal(shape=[784, n_hidden1], stddev=0.1, seed=42)\n",
    "#Creation a variable for weight where all weights for layer 1 will be kept\n",
    "w1 = tf.Variable(initial_value=initial_w1)\n",
    "\n",
    "#setting initial bias (what shifts the activation function from left to the right) and creation the relavnt variable\n",
    "initial_b1 = tf.constant(value=0.0, shape=[n_hidden1])\n",
    "b1 = tf.Variable(initial_value=initial_b1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting the first hidden layer\n",
    "layer1_in = tf.matmul(X, w1) + b1\n",
    "layer1_out = tf.nn.relu(layer1_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting of initial weight 2 \n",
    "initial_w2 = tf.truncated_normal(shape=[n_hidden1, n_hidden2], stddev=0.1, seed=42)\n",
    "w2 = tf.Variable(initial_value=initial_w2)\n",
    "\n",
    "initial_b2 = tf.constant(value=0.0, shape=[n_hidden2])\n",
    "b2 = tf.Variable(initial_value=initial_b2)\n",
    "\n",
    "#Setting the second hidden layer\n",
    "layer2_in = tf.matmul(layer1_out, w2) + b2\n",
    "layer2_out = tf.nn.relu(layer2_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting of final weight 3 (considering the final output (number of classes = 10))\n",
    "initial_w3 = tf.truncated_normal(shape=[n_hidden2, 10], stddev=0.1, seed=42)\n",
    "w3 = tf.Variable(initial_value=initial_w3)\n",
    "\n",
    "initial_b3 = tf.constant(value=0.0, shape=[10])\n",
    "b3 = tf.Variable(initial_value=initial_b3)\n",
    "\n",
    "#Setting the final output layer\n",
    "layer3_in = tf.matmul(layer2_out, w3) + b3\n",
    "output = tf.nn.softmax(layer3_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss, Optimization and Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Defining loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using avarage of losses among bathes using tf.reduce_mean\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=Y, logits=output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Defining optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#including minimazation of our loss and updating our variables (weights and biases)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "train_step = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting index of the maximum value from the output using argmax\n",
    "correct_pred = tf.equal(tf.argmax(output, axis=1), tf.argmax(Y, axis=1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "\n",
    "#to initialise all the variables set above\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### bathching the Data with bacth size = 1000\n",
    "num_examples = y_train.shape[0]\n",
    "nr_iterations = int(num_examples/1000)\n",
    "\n",
    "index_in_epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_batch(batch_size, data, labels):\n",
    "    global num_examples\n",
    "    global index_in_epoch\n",
    "    \n",
    "    start = index_in_epoch\n",
    "    #index_in_epoch = index_in_epoch + batch size\n",
    "    index_in_epoch += batch_size\n",
    "    \n",
    "    if index_in_epoch > num_examples:\n",
    "        start = 0\n",
    "        index_in_epoch = batch_size\n",
    "    end = index_in_epoch\n",
    "    \n",
    "    return data[start:end], labels[start:end]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot feed value of shape (1000,) for Tensor 'Placeholder_1:0', which has shape '(?, 10)'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-ca7ba0037a18>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mfeed_dictionary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mbatch_y\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dictionary\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mbatch_accuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dictionary\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\exts-ml\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    927\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 929\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    930\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\exts-ml\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1126\u001b[0m                              \u001b[1;34m'which has shape %r'\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1127\u001b[0m                              (np_val.shape, subfeed_t.name,\n\u001b[1;32m-> 1128\u001b[1;33m                               str(subfeed_t.get_shape())))\n\u001b[0m\u001b[0;32m   1129\u001b[0m           \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_feedable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1130\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Tensor %s may not be fed.'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0msubfeed_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot feed value of shape (1000,) for Tensor 'Placeholder_1:0', which has shape '(?, 10)'"
     ]
    }
   ],
   "source": [
    "for epoch in range (nr_epochs):\n",
    "    \n",
    "    for i in range(nr_iterations):\n",
    "        \n",
    "        batch_x, batch_y = next_batch(batch_size=1000, data=X_train, labels=y_train)\n",
    "        \n",
    "        #to feed the session, which will run all calculations\n",
    "        feed_dictionary = {X:batch_x, Y:batch_y}\n",
    "        \n",
    "        sess.run(train_step, feed_dict=feed_dictionary)\n",
    "        \n",
    "        batch_accuracy = sess.run(fetches=[accuracy], feed_dict=feed_dictionary)\n",
    "    \n",
    "    print(f'Epoch {epoch} \\t| Training Accuracy = {batch_accuracy}')\n",
    "    \n",
    "print('Done training!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise - Visualize weights\n",
    "\n",
    "> **Exercise**: Plot the weights from the first fully connected layer (the templates) with the `imshow()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "???"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
